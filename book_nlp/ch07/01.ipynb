{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,t_train),(x_test,t_test) = sequence.load_data('addition.txt')\n",
    "char_to_id,id_to_char = sequence.get_vocab()\n",
    "vocab_size  =len(char_to_id)\n",
    "wordec_size = 16\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 25\n",
    "max_grad = 5.0\n",
    "model = Seq2seq(vocab_size,wordec_size,hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_ylim() takes from 1 to 5 positional arguments but 26 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\download\\machine learning\\deep learning\\pytorch_\\book_nlp\\ch07\\01.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch07/01.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     acc_list\u001b[39m.\u001b[39mappend(acc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch07/01.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# print('val acc %.3f%%'%(acc*100))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch07/01.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mplot(acc_list)\n",
      "File \u001b[1;32md:\\download\\machine learning\\deep learning\\pytorch_\\book_nlp\\ch07\\..\\common\\trainer.py:62\u001b[0m, in \u001b[0;36mTrainer.plot\u001b[1;34m(self, ylim)\u001b[0m\n\u001b[0;32m     60\u001b[0m x \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_list))\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m ylim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     plt\u001b[39m.\u001b[39;49mylim(\u001b[39m*\u001b[39;49mylim)\n\u001b[0;32m     63\u001b[0m plt\u001b[39m.\u001b[39mplot(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_list, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     64\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39miterations (x\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_interval) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\application\\anaconda\\lib\\site-packages\\matplotlib\\pyplot.py:1736\u001b[0m, in \u001b[0;36mylim\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs:\n\u001b[0;32m   1735\u001b[0m     \u001b[39mreturn\u001b[39;00m ax\u001b[39m.\u001b[39mget_ylim()\n\u001b[1;32m-> 1736\u001b[0m ret \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mset_ylim(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1737\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mTypeError\u001b[0m: set_ylim() takes from 1 to 5 positional arguments but 26 were given"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train,t_train,max_epoch=1,batch_size=batch_size,max_grad=max_grad)\n",
    "    correct_num=0\n",
    "    for i in range(len(x_test)):\n",
    "        question,correct = x_test[[i]],t_test[[i]]\n",
    "        verbose = i<10\n",
    "        correct_num+=eval_seq2seq(model,question,correct,id_to_char,verbose)\n",
    "    acc = float(correct_num)/len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    # print('val acc %.3f%%'%(acc*100))\n",
    "trainer.plot(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self,vocab_size,wordec_size,hidden_size) -> None:\n",
    "        V,D,H = vocab_size,wordec_size,hidden_size\n",
    "\n",
    "        #初始化参数\n",
    "        embed_w = (np.random.randn(V,D)/100).astype('f')\n",
    "        lstm_Wx = (np.random.randn(D,4*H)/np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H,4*H)/np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "\n",
    "        #编码器的几个层\n",
    "        self.embed = TimeEmbedding(embed_w)\n",
    "        self.lstm = TimeLSTM(lstm_Wx,lstm_Wh,lstm_b,stateful=False)\n",
    "\n",
    "        #参数\n",
    "        self.params = self.embed.params+self.lstm.params\n",
    "        self.grads = self.embed.grads+self.lstm.grads\n",
    "\n",
    "        self.hs = None #每个时间步的隐状态\n",
    "    \n",
    "    def forward(self,xs):\n",
    "        xs = self.embed.forward(xs) #取出某个序列的词向量\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return self.hs[:,-1,:]#返回最后一个时间步的隐藏状态\n",
    "    def backward(self,dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        #初始状态由解码器传来\n",
    "        dhs[:,-1,:] = dh\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        out = self.embed.backward(dout)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self,vocab_size,wordec_size,hidden_size) -> None:\n",
    "        V,D,H = vocab_size,wordec_size,hidden_size\n",
    "        #decoder也要学习：故先初始化参数\n",
    "        embed_w = (np.random.randn(V,D)/100).astype('f')\n",
    "        lstm_Wx = (np.random.randn(D,4*H)/np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H,4*H)/np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "        affine_w = (np.random.randn(H,V)/np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_w)\n",
    "        self.lstm = TimeLSTM(lstm_Wx,lstm_Wh,lstm_b,stateful=False)\n",
    "        self.affine = TimeAffine(affine_w,affine_b)\n",
    "        \n",
    "        self.params = self.embed.params+self.lstm.params+self.affine.params\n",
    "        self.grads = self.embed.params+self.lstm.params+self.affine.params\n",
    "    def forward(self,xs,h):\n",
    "        self.lstm.set_state(h)\n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        out = self.affine.forward(out)\n",
    "        return out\n",
    "    def backward(self,dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh\n",
    "        return dh\n",
    "    def generate(self,h,start_id,sample_size):\n",
    "        self.lstm.set_state(h)\n",
    "        sample = []\n",
    "        sample_id = start_id#当前预测时的输入，把前一个预测结果当作下一次预测的输出\n",
    "        for _ in range(sample_size):#训练数据获得输出\n",
    "            x = np.array(sample_id).reshape(1,1)\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            y = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(y.flatten())\n",
    "            sample.append(int(sample_id))\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq:\n",
    "    def __init__(self,vocab_size,wordec_size,hidden_size) -> None:\n",
    "        V,D,H = vocab_size,wordec_size,hidden_size\n",
    "        self.encoder = Encoder(V,D,H)\n",
    "        self.decoder = Decoder(V,D,H)\n",
    "        self.loss = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params+self.decoder.params\n",
    "        self.grads = self.encoder.params+self.decoder.params\n",
    "    def forward(self,xs,ts):\n",
    "        decoder_xs = ts[:,:-1] #训练时使用，包括_\n",
    "        decoder_ts = ts[:,1:]#判断正误，计算损失，即应该是正确结果，去掉第一个_\n",
    "\n",
    "        h = self.encoder.forward(xs)\n",
    "        out = self.decoder.forward(decoder_xs,h)#喂入正确的数据训练\n",
    "        loss = self.loss.forward(out,decoder_ts)\n",
    "        return loss\n",
    "    def backward(self,dout=1):\n",
    "        dout = self.loss.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n",
    "    def generate(self,xs,start_id,sample_size):\n",
    "        h = self.encoder.forward(xs)#先用编码器训练数据得到h\n",
    "        sample = self.decoder.generate(h,start_id,sample_size)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,t_train),(x_test,t_test) = sequence.load_data('addition.txt')\n",
    "char_to_id,id_to_char = sequence.get_vocab()\n",
    "vocab_size  =len(char_to_id)\n",
    "wordec_size = 16\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 25\n",
    "max_grad = 5.0\n",
    "model = seq2seq(vocab_size,wordec_size,hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train,t_train,max_epoch=1,batch_size=batch_size,max_grad=max_grad)\n",
    "    correct_num=0\n",
    "    for i in range(len(x_test)):\n",
    "        question,correct = x_test[[i]],t_test[[i]]\n",
    "        verbose = i<10\n",
    "        correct_num+=eval_seq2seq(model,question,correct,id_to_char,verbose)\n",
    "    acc = float(correct_num)/len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%'%(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn\n",
    "arr = ((2,2) /100).astype('f')\n",
    "arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6304eda03dbff4706a59b217e3d49660986a8cfe2c8470eb4f922b8901932d20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
