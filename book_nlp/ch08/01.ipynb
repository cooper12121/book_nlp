{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nattention模型\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "attention模型\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "from common.np import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    \"\"\" 根据编码器的输出hs(所有时间序列的隐状态)，求出各个单词的权重a ,使用softmax使得权重都在0-1之间\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None  # 存储中间结果，即hs，和计算a需要用到的初始化矩阵hr\n",
    "\n",
    "    # h:解码器当前时间步的隐状态：目标是计算h和hs中各个单词的相似度，即权重a,有a和hs加权和得到上下文向量c,加权和即可确定当前词h应该注意到hs中的哪些词\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs*hr\n",
    "        s = np.sum(t, axis=2)  # N,T 得到每个时间步的权重,需要正规化\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = [hs, hr]\n",
    "        return a  # 得到了权重a (N,T) 针对encoder中的每一个时间步的权重\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        # 反向传播，反着求即可\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt*hr\n",
    "        dhr = dt*hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        return dhs, dh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    \"\"\" 计算权重a和hs的加权和 \n",
    "    得到的是一个时间步的上下文向量c:N,1,H\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        # 计算加权和最好方式就是矩阵dot\n",
    "        N, T, H = hs.shape\n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs*ar\n",
    "        c = np.sum(t, axis=1)  # t.shape:[N,H]\n",
    "        self.cache = (hs, ar)\n",
    "\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt*hs\n",
    "        dhs = dt*ar\n",
    "        da = np.sum(da, axis=2)\n",
    "\n",
    "        return dhs, da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    \"\"\" 整合了上两个模块 \n",
    "    单个时间步的attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weightsum_layer = WeightSum()\n",
    "        self.attention_weight = None  # 保存当前的注意力权重\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        c = self.weightsum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        dhs0, da = self.weightsum_layer.backward(dc)  # 该层正向传播是有a,hs得到c,故反向传播是由dc得到dhs,da\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "\n",
    "        dhs = dhs0+dhs1\n",
    "        return dhs, dh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    \"\"\" 只是组合了多个attention层 \n",
    "        每个attention各自单独进行反向传播和正向传播\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None  # 保存每一个时间步的上下文c\n",
    "        self.attention_weight = None  # 保存每一个时间步的权重a\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        \"\"\" \n",
    "        encoder的hs送到每一个attention \n",
    "        对于解码器每一个时间步都生成c\n",
    "        \"\"\"\n",
    "        N, T, H = hs_dec.shape\n",
    "        out_c = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weight = []\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out_c[:, t, :] = layer.forward(\n",
    "                hs_enc, hs_dec[:, t, :])  # 得到当前时间步的c\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weight.append(layer.attention_weight)\n",
    "        return out_c\n",
    "\n",
    "    def backward(self, dout_c):\n",
    "        # 正向传播是由hs_enc,hs_dec得到out_c,故反向传播是由dout_c得到dhs_enc,dhs_dec\n",
    "        N, T, H = dout_c.shape\n",
    "        dhs_enc = 0  # 因为解码器所有层用的都是同一个dhs_enc,故最终的dhs_enc是多有的dhs_enc之和\n",
    "        dhs_dec = np.empty_like(dout_c)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout_c)\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "        return dhs_enc, dhs_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from common.time_layers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordec_size, hidden_size) -> None:\n",
    "        V, D, H = vocab_size, wordec_size, hidden_size\n",
    "\n",
    "        # 初始化参数\n",
    "        embed_w = (np.random.randn(V, D)/100).astype('f')\n",
    "        lstm_Wx = (np.random.randn(D, 4*H)/np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H, 4*H)/np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "\n",
    "        # 编码器的几个层\n",
    "        self.embed = TimeEmbedding(embed_w)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "\n",
    "        # 参数\n",
    "        self.params = self.embed.params+self.lstm.params\n",
    "        self.grads = self.embed.grads+self.lstm.grads\n",
    "\n",
    "        self.hs = None  # 每个时间步的隐状态\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)  # 取出某个序列的词向量\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return self.hs[:, -1, :]  # 返回最后一个时间步的隐藏状态\n",
    "\n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        # 初始状态由解码器传来\n",
    "        dhs[:, -1, :] = dh\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        out = self.embed.backward(dout)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordec_size, hidden_size) -> None:\n",
    "        V, D, H = vocab_size, wordec_size, hidden_size\n",
    "        # decoder也要学习：故先初始化参数\n",
    "        embed_w = (np.random.randn(V, D)/100).astype('f')\n",
    "        lstm_Wx = (np.random.randn(D, 4*H)/np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H, 4*H)/np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "        affine_w = (np.random.randn(H, V)/np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_w)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "        self.affine = TimeAffine(affine_w, affine_b)\n",
    "\n",
    "        self.params = self.embed.params+self.lstm.params+self.affine.params\n",
    "        self.grads = self.embed.params+self.lstm.params+self.affine.params\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        self.lstm.set_state(h)\n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        out = self.affine.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        self.lstm.set_state(h)\n",
    "        sample = []\n",
    "        sample_id = start_id  # 当前预测时的输入，把前一个预测结果当作下一次预测的输出\n",
    "        for _ in range(sample_size):  # 训练数据获得输出\n",
    "            x = np.array(sample_id).reshape(1, 1)\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            y = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(y.flatten())\n",
    "            sample.append(int(sample_id))\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 带attention的seq2seq实现 \n",
    "与之前encoder不同之处仅在于返回的是全部的隐藏状态hs\n",
    "decoder不同之处在于添加了attention层：添加的位置可由自己决定\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dxs = self.embed.backward(dout)\n",
    "        return dxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:  # 不能继承，因为它在中间添加了一个新的层\n",
    "    def __init__(self, vocab_size, wordec_size, hidden_size) -> None:\n",
    "        V, D, H = vocab_size, wordec_size, hidden_size\n",
    "        embed_w = (np.random.randn(V, D)/100).astype('f')\n",
    "        lstm_Wx = (np.random.randn(D, 4*H)/np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H, 4*H)/np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "        affine_w = (np.random.randn(2*H, V)/np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        # 初始化参数\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_w)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        # 添加attention层\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_w, affine_b)\n",
    "\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, hs_enc):\n",
    "        # lstm只需要编码器的最后一个隐藏状态\n",
    "        h = hs_enc[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        hs_dec = self.lstm.forward(out)\n",
    "        c = self.attention.forward(hs_enc, hs_dec)\n",
    "        # 隐藏向量和上下文向量拼接后一起作为affine的输入\n",
    "        affine_in = np.concatenate(hs_dec, c, axis=2)  # N,T,H,应该在h维拼接\n",
    "        score = self.affine.forward(affine_in)\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        daffine_in = self.affine.backward(dscore)\n",
    "        N, T, H2 = daffine_in.shape\n",
    "        H  = H2//2 #注意H2是拼接之后的\n",
    "        dc = daffine_in[:, :, :H]  # 左开右闭\n",
    "        dhs_dec_0 = daffine_in[:, :, H:]\n",
    "        dhs_enc, dhs_dec_1 = self.attention.backward(dc)\n",
    "        dhs_dec = dhs_dec_0+dhs_dec_1\n",
    "        dout = self.lstm.backward(dhs_dec)\n",
    "        dxs = self.embed.backward(dout)\n",
    "        return dhs_enc\n",
    "    def generate(self,enc_hs,start_id,sample_size):\n",
    "        self.lstm.set_state(enc_hs)\n",
    "        sample = []\n",
    "        sample_id = start_id\n",
    "        for _ in range(sample_size):\n",
    "            #取得开始单词的词向量，训练，然后生成\n",
    "            x = np.array(sample_id).reshape(1,1)\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            out = self.attention.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            y = np.argmax(score.flatten())\n",
    "            sample_id = y #当前的生成做为下一个的输入\n",
    "            sample.append(int(sample_id))\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2Seq:\n",
    "    def __init__(self,vocab_size, wordec_size, hidden_size) -> None:\n",
    "        self.encoder = AttentionEncoder(vocab_size, wordec_size, hidden_size)\n",
    "        self.decoder = AttentionDecoder(vocab_size, wordec_size, hidden_size)\n",
    "        #损失函数，计算损失\n",
    "        self.loss = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params,self.grads = [],[]\n",
    "\n",
    "        self.params = self.encoder.params+self.decoder.params\n",
    "        self.grads = self.encoder.grads+self.decoder.params\n",
    "    def forward(self,xs,ts):\n",
    "        decoder_xs,decoder_ts = ts[:,:-1],ts[:,1:] #训练时包括所有，判断正误不用_\n",
    "        enc_hs = self.encoder.forward(xs)\n",
    "        out = self.decoder.forward(decoder_xs,enc_hs)\n",
    "        loss = self.loss.forward(out,decoder_ts)\n",
    "        return loss\n",
    "    def backward(self,dout=1):\n",
    "        dout = self.loss.backward(dout)\n",
    "        denc_hs = self.decoder.backward(dout)\n",
    "        dxs = self.encoder.backward(denc_hs)\n",
    "        return dxs\n",
    "    def generate(self,xs,start_id,sample_size):\n",
    "        enc_hs = self.encoder.forward(xs)\n",
    "        sample = self.decoder.generate(enc_hs,start_id,sample_size)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.util import clip_grads\n",
    "import matplotlib.pyplot as plt\n",
    "# from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,model,optimizer) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = []\n",
    "        self.current_epoch=0\n",
    "        self.eval_interval = None\n",
    "    def fit(self,train_data,table_data,batch_size,max_epochs,max_grad,eval_interval=20):#迭代多少组时打印当前情况\n",
    "        data_size = len(train_data)\n",
    "        max_iters = data_size//batch_size #数据的批次大小\n",
    "        loss_sum = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start = time.time()\n",
    "        for epoch in range(max_epochs):\n",
    "            #打乱数据，然后计算每一个批次的数据\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = train_data[idx]\n",
    "            t = table_data[idx]#已经打乱的数据\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                #取出一个批次\n",
    "                batch_x = x[batch_size*iters:batch_size*(iters+1)]\n",
    "                batch_t = t[batch_size*iters:batch_size*(iters+1)]\n",
    "\n",
    "                #前向传播计算损失、后向传播求梯度更新参数\n",
    "                loss = self.model.forward(batch_x,batch_t) #model的前向传播结束时已经计算出了损失\n",
    "                self.model.backward()\n",
    "                params,grads = self.model.params,self.model.grads\n",
    "\n",
    "                if max_grad is not None:#梯度裁剪\n",
    "                    clip_grads(grads,max_grad)\n",
    "                self.optimizer.update(params,grads)\n",
    "                loss_sum+=loss\n",
    "                loss_count+=1\n",
    "            \n",
    "                #评价\n",
    "                if eval_interval is not None and (iters%eval_interval==0):\n",
    "                    avg_loss = loss_sum/loss_count\n",
    "                    cost_time = time.time()-start\n",
    "                    print('| epoch %d| | iters %d/%d | | time %d | | avg_loss %d |'%(epoch,iters,max_iters,cost_time,avg_loss))\n",
    "                    self.loss.append(float(avg_loss))\n",
    "                    loss_count,loss_sum=0,0 #一定模块打断，也就是将数据分成了几个单独的模块计算\n",
    "            self.current_epoch+=1\n",
    "        def train_plot(self,ylim=None):\n",
    "            x = np.arange(len(self.loss)) #x轴，self.loss为y轴\n",
    "            if ylim is not None:\n",
    "                plt.ylim(ylim)\n",
    "            plt.plot(x,self.loss,label=\"train_data\")\n",
    "            plt.xlabel('iterations (x'+str(self.eval_interval)+')')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "(x_train,t_train),(x_test,t_test) = sequence.load_data('date.txt')\n",
    "char_to_id,id_to_char = sequence.get_vocab()\n",
    "x_train,x_test = x_train[:,::-1],x_test[:,::-1]#反转\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\download\\machine learning\\deep learning\\pytorch_\\book_nlp\\ch08\\01.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#初始化模型\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m AttentionSeq2Seq(vocab_size, wordec_size, hidden_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39;49mforward(x_train,t_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer \u001b[39m=\u001b[39m Adam()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model,optimizer)\n",
      "\u001b[1;32md:\\download\\machine learning\\deep learning\\pytorch_\\book_nlp\\ch08\\01.ipynb Cell 16\u001b[0m in \u001b[0;36mAttentionSeq2Seq.forward\u001b[1;34m(self, xs, ts)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m decoder_xs,decoder_ts \u001b[39m=\u001b[39m ts[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],ts[:,\u001b[39m1\u001b[39m:] \u001b[39m#训练时包括所有，判断正误不用_\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m enc_hs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mforward(xs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mforward(decoder_xs,enc_hs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mforward(out,decoder_ts)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[1;32md:\\download\\machine learning\\deep learning\\pytorch_\\book_nlp\\ch08\\01.ipynb Cell 16\u001b[0m in \u001b[0;36mAttentionDecoder.forward\u001b[1;34m(self, xs, hs_enc)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed\u001b[39m.\u001b[39mforward(xs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m hs_dec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm\u001b[39m.\u001b[39mforward(out)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention\u001b[39m.\u001b[39;49mforward(hs_enc, hs_dec)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# 隐藏向量和上下文向量拼接后一起作为affine的输入\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m affine_in \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(hs_dec, c, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# N,T,H,应该在h维拼接\u001b[39;00m\n",
      "\u001b[1;32md:\\download\\machine learning\\deep learning\\pytorch_\\book_nlp\\ch08\\01.ipynb Cell 16\u001b[0m in \u001b[0;36mTimeAttention.forward\u001b[1;34m(self, hs_enc, hs_dec)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     layer \u001b[39m=\u001b[39m Attention()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     out_c[:, t, :] \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         hs_enc, hs_dec[:, t, :])  \u001b[39m# 得到当前时间步的c\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mappend(layer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_weight\u001b[39m.\u001b[39mappend(layer\u001b[39m.\u001b[39mattention_weight)\n",
      "\u001b[1;32md:\\download\\machine learning\\deep learning\\pytorch_\\book_nlp\\ch08\\01.ipynb Cell 16\u001b[0m in \u001b[0;36mAttention.forward\u001b[1;34m(self, hs, h)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hs, h):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_weight_layer\u001b[39m.\u001b[39;49mforward(hs, h)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweightsum_layer\u001b[39m.\u001b[39mforward(hs, a)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_weight \u001b[39m=\u001b[39m a\n",
      "\u001b[1;32md:\\download\\machine learning\\deep learning\\pytorch_\\book_nlp\\ch08\\01.ipynb Cell 16\u001b[0m in \u001b[0;36mAttentionWeight.forward\u001b[1;34m(self, hs, h)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hs, h):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     N, T, H \u001b[39m=\u001b[39m hs\u001b[39m.\u001b[39mshape\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     hr \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39;49mreshape(N, \u001b[39m1\u001b[39;49m, H)\u001b[39m.\u001b[39;49mrepeat(T, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     t \u001b[39m=\u001b[39m hs\u001b[39m*\u001b[39mhr\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/download/machine%20learning/deep%20learning/pytorch_/book_nlp/ch08/01.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     s \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(t, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# N,T 得到每个时间步的权重,需要正规化\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#设定超参数\n",
    "vocab_size=len(char_to_id)#词表大小\n",
    "wordec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size,epochs = 128,10\n",
    "max_grad = 5.0 #梯度裁剪\n",
    "\n",
    "#初始化模型\n",
    "model = AttentionSeq2Seq(vocab_size, wordec_size, hidden_size)\n",
    "model.forward(x_train,t_train)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6304eda03dbff4706a59b217e3d49660986a8cfe2c8470eb4f922b8901932d20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
